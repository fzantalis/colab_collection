{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mozilla_TTS_WaveRNN_ttmai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fzantalis/colab_collection/blob/master/Mozilla_TTS_WaveRNN_ttmai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6lqayVYvCyQ",
        "colab_type": "text"
      },
      "source": [
        "# Text-to-Speech with Mozilla Tacotron+WaveRNN\n",
        "\n",
        "Αυτό είναι ένα notebook που συνδυάζει τα project [mozilla/TTS](https://github.com/mozilla/TTS/) και [erogol/WaveRNN](https://github.com/erogol/WaveRNN) με σκοπό να παρέχει υπηρεσίες TTS.\n",
        "\n",
        "\n",
        "## Install Mozilla TTS and WaveRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Hdw1q_X8JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "git_repo_url = 'https://github.com/mozilla/TTS.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  !git clone -q {git_repo_url}\n",
        "  !cd {project_name} && git checkout Tacotron2-iter-260K-824c091\n",
        "  !pip install -q gdown lws librosa Unidecode==0.4.20 tensorboardX git+git://github.com/bootphon/phonemizer@master localimport\n",
        "  !apt-get install -y espeak\n",
        "git_repo_url = 'https://github.com/erogol/WaveRNN.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  !git clone -q {git_repo_url}\n",
        "  !cd {project_name} && git checkout 8a1c152 && pip install -q -r requirements.txt\n",
        "\n",
        "  \n",
        "import sys\n",
        "sys.path.append('TTS')\n",
        "sys.path.append('WaveRNN')\n",
        "from localimport import localimport\n",
        "  \n",
        "from IPython.display import Audio, display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_D59k2x_uGW",
        "colab_type": "text"
      },
      "source": [
        "## Κατέβασμα των εκπαιδευμένων μοντέλων"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klsVLR6w_u4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WaveRNN\n",
        "!mkdir -p wavernn_models tts_models\n",
        "wavernn_pretrained_model = 'wavernn_models/checkpoint_433000.pth.tar'\n",
        "if not exists(wavernn_pretrained_model):\n",
        "  !gdown -O {wavernn_pretrained_model} https://drive.google.com/uc?id=1frsrKdRbCVYtwS8rZwxpz7tqtdH1p8bB\n",
        "wavernn_pretrained_model_config = 'wavernn_models/config.json'\n",
        "if not exists(wavernn_pretrained_model_config):\n",
        "  !gdown -O {wavernn_pretrained_model_config} https://drive.google.com/uc?id=1kiAGjq83wM3POG736GoyWOOcqwXhBulv\n",
        "    \n",
        "# TTS\n",
        "tts_pretrained_model = 'tts_models/checkpoint_261000.pth.tar'\n",
        "if not exists(tts_pretrained_model):\n",
        "  !gdown -O {tts_pretrained_model} https://drive.google.com/uc?id=1Gl_oxWx2gAIbyuUfepmCXBKzYMT1n10x\n",
        "tts_pretrained_model_config = 'tts_models/config.json'\n",
        "if not exists(tts_pretrained_model_config):\n",
        "  !gdown -O {tts_pretrained_model_config} https://drive.google.com/uc?id=1IJaGo0BdMQjbnCcOL4fPOieOEWMOsXE-"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDaahAVNENpT",
        "colab_type": "text"
      },
      "source": [
        "## Αρχικοποίηση Μοντέλων"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft1LHHdkA2Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# this code is copied from: https://github.com/mozilla/TTS/blob/master/notebooks/Benchmark.ipynb\n",
        "#\n",
        "\n",
        "import io\n",
        "import torch \n",
        "import time\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from matplotlib import pylab as plt\n",
        "import IPython\n",
        "\n",
        "%pylab inline\n",
        "rcParams[\"figure.figsize\"] = (16,5)\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "from TTS.models.tacotron import Tacotron \n",
        "from TTS.layers import *\n",
        "from TTS.utils.data import *\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.utils.generic_utils import load_config, setup_model\n",
        "from TTS.utils.text import text_to_sequence\n",
        "from TTS.utils.synthesis import synthesis\n",
        "from TTS.utils.visual import visualize\n",
        "\n",
        "def tts(model, text, CONFIG, use_cuda, ap, use_gl, speaker_id=None, figures=True):\n",
        "    t_1 = time.time()\n",
        "    waveform, alignment, mel_spec, mel_postnet_spec, stop_tokens = synthesis(model, text, CONFIG, use_cuda, ap, truncated=True, enable_eos_bos_chars=CONFIG.enable_eos_bos_chars)\n",
        "    if CONFIG.model == \"Tacotron\" and not use_gl:\n",
        "        mel_postnet_spec = ap.out_linear_to_mel(mel_postnet_spec.T).T\n",
        "    if not use_gl:\n",
        "        waveform = wavernn.generate(torch.FloatTensor(mel_postnet_spec.T).unsqueeze(0).cuda(), batched=batched_wavernn, target=11000, overlap=550)\n",
        "\n",
        "    print(\" >  Run-time: {}\".format(time.time() - t_1))\n",
        "    if figures:                                                                                                         \n",
        "        visualize(alignment, mel_postnet_spec, stop_tokens, text, ap.hop_length, CONFIG, mel_spec)                                                                       \n",
        "    IPython.display.display(Audio(waveform, rate=CONFIG.audio['sample_rate']))  \n",
        "    #os.makedirs(OUT_FOLDER, exist_ok=True)\n",
        "    #file_name = text.replace(\" \", \"_\").replace(\".\",\"\") + \".wav\"\n",
        "    #out_path = os.path.join(OUT_FOLDER, file_name)\n",
        "    #ap.save_wav(waveform, out_path)\n",
        "    return alignment, mel_postnet_spec, stop_tokens, waveform\n",
        "  \n",
        "use_cuda = True\n",
        "batched_wavernn = True\n",
        "\n",
        "# initialize TTS\n",
        "CONFIG = load_config(tts_pretrained_model_config)\n",
        "from TTS.utils.text.symbols import symbols, phonemes\n",
        "# load the model\n",
        "num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n",
        "model = setup_model(num_chars, CONFIG)\n",
        "# load the audio processor\n",
        "ap = AudioProcessor(**CONFIG.audio)         \n",
        "# load model state\n",
        "if use_cuda:\n",
        "    cp = torch.load(tts_pretrained_model)\n",
        "else:\n",
        "    cp = torch.load(tts_pretrained_model, map_location=lambda storage, loc: storage)\n",
        "\n",
        "# load the model\n",
        "model.load_state_dict(cp['model'])\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "model.eval()\n",
        "print(cp['step'])\n",
        "model.decoder.max_decoder_steps = 2000\n",
        "\n",
        "# initialize WaveRNN\n",
        "VOCODER_CONFIG = load_config(wavernn_pretrained_model_config)\n",
        "with localimport('/content/WaveRNN') as _importer:\n",
        "  from models.wavernn import Model\n",
        "bits = 10\n",
        "\n",
        "wavernn = Model(\n",
        "        rnn_dims=512,\n",
        "        fc_dims=512,\n",
        "        mode=\"mold\",\n",
        "        pad=2,\n",
        "        upsample_factors=VOCODER_CONFIG.upsample_factors,  # set this depending on dataset\n",
        "        feat_dims=VOCODER_CONFIG.audio[\"num_mels\"],\n",
        "        compute_dims=128,\n",
        "        res_out_dims=128,\n",
        "        res_blocks=10,\n",
        "        hop_length=ap.hop_length,\n",
        "        sample_rate=ap.sample_rate,\n",
        "    ).cuda()\n",
        "check = torch.load(wavernn_pretrained_model)\n",
        "wavernn.load_state_dict(check['model'])\n",
        "if use_cuda:\n",
        "    wavernn.cuda()\n",
        "wavernn.eval()\n",
        "print(check['step'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anTcrJwwvZAr",
        "colab_type": "text"
      },
      "source": [
        "## Εδώ γράφουμε την πρόταση που θέλουμε να μετατρέψουμε σε ομιλία"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkxJ8J4dveaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SENTENCE = 'Everything you see right now, everything you listen, is a hundred percent generated by artificial intelligence. You\\'ve seen the movie, right? The one where the man has this computer that was programmed to be all-knowing, and it gave him all the answers?  That\\'s the future.  The future is going to be an absolute nightmare.'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuegqdoVvk14",
        "colab_type": "text"
      },
      "source": [
        "## Μετατροπή κειμένου σε Ομιλία"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx93hVb6Y8dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "align, spec, stop_tokens, wav = tts(model, SENTENCE, CONFIG, use_cuda, ap, speaker_id=0, use_gl=False, figures=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}