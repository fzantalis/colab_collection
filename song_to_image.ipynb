{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fzantalis/colab_collection/blob/master/song_to_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb5a4b79-cc61-40aa-ae3d-6c53a5265b49",
      "metadata": {
        "id": "eb5a4b79-cc61-40aa-ae3d-6c53a5265b49"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.5.0+cu121 torchvision==0.20.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install gradio transformers diffusers accelerate bitsandbytes\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5bb907e-a9d6-4faa-afae-d855cb9bcaa9",
      "metadata": {
        "id": "b5bb907e-a9d6-4faa-afae-d855cb9bcaa9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cdb7a6f-c243-46fb-9a51-89f27382e480",
      "metadata": {
        "id": "8cdb7a6f-c243-46fb-9a51-89f27382e480"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token <replace_with_your_huggingface_token>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79cf0789-b9c6-4ce9-bcf8-26845f04a53d",
      "metadata": {
        "id": "79cf0789-b9c6-4ce9-bcf8-26845f04a53d"
      },
      "outputs": [],
      "source": [
        "!rm -rf TotoroUI\n",
        "!git clone -b totoro3 https://github.com/camenduru/ComfyUI TotoroUI\n",
        "%cd /workspace/mywork/fzantalis/stable_diffusion/TotoroUI\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post2\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors -d models/unet -o flux1-dev-fp8.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d models/vae -o ae.sft\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d models/clip -o clip_l.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro_extras import nodes_post_processing\n",
        "from totoro import model_management\n",
        "\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "VAEEncode = NODE_CLASS_MAPPINGS[\"VAEEncode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "ImageScaleToTotalPixels = nodes_post_processing.NODE_CLASS_MAPPINGS[\"ImageScaleToTotalPixels\"]()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "    unet = UNETLoader.load_unet(\"flux1-dev-fp8.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8947f196-bdde-4632-9780-1cc98184edb4",
      "metadata": {
        "id": "8947f196-bdde-4632-9780-1cc98184edb4"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -O /usr/local/bin/yt-dlp\n",
        "!chmod a+rx /usr/local/bin/yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f30a832-e4ea-4a15-8332-852261385635",
      "metadata": {
        "id": "9f30a832-e4ea-4a15-8332-852261385635"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
        "import subprocess\n",
        "import os\n",
        "import gc  # Import garbage collector module\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "#USE THIS IF YOU WANT TO AUTO DOWNLOAD THE SONG FROM YOUTUBE AND EXTRACT LYRICS TO ENGLISH AUTOMATICALLY WITH WHISPER\n",
        "youtube_link=\"https://www.youtube.com/watch?v=D0Dx8LvX9Fs\"\n",
        "phrase_num=5\n",
        "def extract_phrases(response, type):\n",
        "    phrases = []\n",
        "    lines = response.strip().split('\\n')\n",
        "    lines = lines[14:]\n",
        "    stylized_section = False\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        # Check if we've reached the \"STYLIZED PHRASES:\" section\n",
        "        if line.upper() == type + \":\":\n",
        "            stylized_section = True\n",
        "            continue\n",
        "        # Skip lines until we reach the \"STYLIZED PHRASES:\" section\n",
        "        if not stylized_section:\n",
        "            continue\n",
        "        # Extract phrases in the \"STYLIZED PHRASES:\" section\n",
        "        if line[0].isdigit() and line[1] == '.':\n",
        "            # Remove numbering and any leading/trailing whitespace\n",
        "            phrase = line.split('.', 1)[1].strip()\n",
        "            phrases.append(phrase)\n",
        "            # Stop after collecting 4 phrases\n",
        "            if len(phrases) >= phrase_num:\n",
        "                break\n",
        "    return phrases\n",
        "\n",
        "import gc\n",
        "import subprocess\n",
        "\n",
        "# Define the output file\n",
        "output_file = \"downloaded_audio.mp3\"\n",
        "!rm  $output_file\n",
        "# Download the audio using yt-dlp\n",
        "# Build the command as a list of arguments\n",
        "yt_dlp_command = [\n",
        "    \"yt-dlp\",\n",
        "    \"--restrict-filenames\",\n",
        "    \"-x\",\n",
        "    \"--audio-format\",\n",
        "    \"mp3\",\n",
        "    \"--output\",\n",
        "    output_file,\n",
        "    youtube_link\n",
        "]\n",
        "#yt-dlp --restrict-filenames -x --audio-format mp3 --output $output_file $youtube_link\n",
        "\n",
        "# Run the command\n",
        "result = subprocess.run(yt_dlp_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "if result.returncode != 0:\n",
        "    # There was an error downloading the audio\n",
        "    error_message = result.stderr.decode('utf-8')\n",
        "    print(f\"Error downloading audio: {error_message}\")\n",
        "# Now, pass the audio file to Whisper\n",
        "audio_input = output_file\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "model_id = \"openai/whisper-large-v2\"\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
        ")\n",
        "model.to(device)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Create the pipeline\n",
        "whisper_model = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Transcribe the audio\n",
        "transcription = whisper_model(audio_input, generate_kwargs={\"language\": \"english\", \"return_timestamps\": True})[\"text\"]\n",
        "lyrics = transcription\n",
        "print(lyrics)\n",
        "\n",
        "# Clean up\n",
        "del whisper_model.model\n",
        "del whisper_model.tokenizer\n",
        "del whisper_model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a8d306-386a-481a-a80b-9a772e620817",
      "metadata": {
        "id": "56a8d306-386a-481a-a80b-9a772e620817"
      },
      "outputs": [],
      "source": [
        "# REPLACE THE TEXT BELOW WITH ENGLISH LYRICS IF YOU WANT TO SKIP THE ABOVE STEP\n",
        "lyrics = \"\"\"PAST HERE ENGLISH LYRICS\n",
        "IN PLAIN TEXT IF YOU WANT TO SKIP THE DOWNLOAD\n",
        "PART ABOVE WITH THE WHISPER MODEL.\n",
        "THIS IS FASTER BUT YOU NEED TO HAVE THE LYRICS IN ENGLISH\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe2da006-2eeb-47b0-9a11-1851b3eac4db",
      "metadata": {
        "id": "fe2da006-2eeb-47b0-9a11-1851b3eac4db"
      },
      "outputs": [],
      "source": [
        "# Load LLaMA model inside the function using the new pipeline method\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "text_gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "phrase_num=5\n",
        "# Use LLaMA model to extract 4 best phrases\n",
        "prompt = f\"\"\"This is a conversation between a chatbot expert in arts and a user. The user will provide lyrics to the chatbot and the chatbot will extract the {phrase_num} best most inspiring phrases from the lyrics.\n",
        "Don't hesitate to extract longer phrases in order to maintain\n",
        "Then the chatbot will try to interpret the meaning of the phrases in order to transform them into a prompt for a text to image generation model.\n",
        "The chatbot will only provide a list of {phrase_num} phrases, stylized as asked by the user, focusing on the artistic aspects of the picture. The output will be like that:\n",
        "ORIGINAL PHRASES:\n",
        "1. example phrase 1\n",
        "2. example phrase 2\n",
        "...\n",
        "\n",
        "STYLIZED PHRASES:\n",
        "1. stylized phrase 1\n",
        "2. stylized phrase 2\n",
        "...\n",
        "\n",
        "User: Hi chatbot I need some help.\n",
        "Chatbot: Hi user, how can I help you?\n",
        "\n",
        "User: Extract the {phrase_num} best most inspiring phrases from the following song lyrics:\n",
        "\n",
        "{lyrics}\n",
        "\n",
        "Then rephrase them to be suitable as prompts for a text-to-image model.\n",
        "For example, if the phrase is 'Broken like a membrane, we burst and we burst one by one'\n",
        "you could rephrase it like 'A watercolor painting of a broken membrane symbolizing fragility, with figures bursting into stardust one by one, ethereal atmosphere, soft glow lighting, in the style of Salvador Dalí, high definition, intricate details, trending on ArtStation.'\n",
        "Provide the {phrase_num} phrases as a numbered list.\n",
        "Don't be too verbose. Only give me back the list of the {phrase_num} phrases.\n",
        "ra\n",
        "Chatbot:\n",
        "\"\"\"\n",
        "\n",
        "response = text_gen_pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=1000,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    num_return_sequences=1,\n",
        ")[0]['generated_text']\n",
        "\n",
        "# Free up memory used by the text generation pipeline\n",
        "del text_gen_pipeline.model\n",
        "del text_gen_pipeline.tokenizer\n",
        "del text_gen_pipeline\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "orig_phrases = extract_phrases(response, \"ORIGINAL PHRASES\")\n",
        "\n",
        "# Debug: Print the phrases to console\n",
        "print(\"Extracted Phrases:\")\n",
        "for idx, orig_phrase in enumerate(orig_phrases, 1):\n",
        "    print(f\"{idx}. {orig_phrase}\")\n",
        "\n",
        "phrases = extract_phrases(response, \"STYLIZED PHRASES\")\n",
        "# Debug: Print the phrases to console\n",
        "print(\"Stylized Phrases:\")\n",
        "for idx, phrases in enumerate(phrases, 1):\n",
        "    print(f\"{idx}. {phrases}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43b8213-8e54-44c1-a5b4-bfdfe9a42367",
      "metadata": {
        "id": "b43b8213-8e54-44c1-a5b4-bfdfe9a42367"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
        "import subprocess\n",
        "import os\n",
        "import gc  # Import garbage collector module\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "counter = 0\n",
        "phrases = extract_phrases(response, \"STYLIZED PHRASES\")\n",
        "for positive_prompt in phrases:\n",
        "    with torch.inference_mode():\n",
        "        width = 1024\n",
        "        height = 1024\n",
        "        seed = 0\n",
        "        steps = 20\n",
        "        sampler_name = \"euler\"\n",
        "        scheduler = \"simple\"\n",
        "\n",
        "        if seed == 0:\n",
        "            seed = random.randint(0, 18446744073709551615)\n",
        "        print(seed)\n",
        "\n",
        "        print(positive_prompt)\n",
        "        cond, pooled = clip.encode_from_tokens(clip.tokenize(positive_prompt), return_pooled=True)\n",
        "        cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "        noise = RandomNoise.get_noise(seed)[0]\n",
        "        guider = BasicGuider.get_guider(unet, cond)[0]\n",
        "        sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "        sigmas = BasicScheduler.get_sigmas(unet, scheduler, steps, 1.0)[0]\n",
        "        latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "        sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "        model_management.soft_empty_cache()\n",
        "        decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "        Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(str(counter)+\".png\")\n",
        "        img_array = np.array(decoded * 255, dtype=np.uint8)[0]\n",
        "        img = Image.fromarray(img_array)\n",
        "        display(img)\n",
        "        counter += 1\n",
        "# Free up memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}